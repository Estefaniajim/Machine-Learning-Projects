{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification of sentiment in a product review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A classifier takes some input x, it pushes it through what's called a model to output some value y, that we're trying to predict.\n",
    "In computer vision, we do a lot of classification. We'll take an image, and figure out what is in that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a prediction whether it is a positive sentence or a negative sentence. In the sentimental analysis you can imagine a simple kind of threshold classifier. A threshold classifiers have some limitations.  how do we weigh different words? a linear classifier, instead of having a list of positive and negative words, actually takes all the words and adds weights to them. If you know the weight of each word, then it is a linear classifier because the output is basically the weighted sum of the input. Just weight, what features appears, what words appear in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in summary, given a sentence and given the weights for\n",
    "the sentence, what we do is compute the score,\n",
    "which is the weighted count of the words that appear in the sentence.\n",
    "And then we say if the score is greater than zero,\n",
    "we predict y-hat to be positive.\n",
    "While if the score is less than zero, we predict it to be negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand decision boundaries,\n",
    "suppose you only had two words with non-zero weight.\n",
    "You have awesome with positive weight of one and\n",
    "awful which is just awful so it has a negative weight of 1.5.\n",
    "If you have this situation then the score is gonna be 1 times the number of\n",
    "awesomes in the sentence, minus 1.5 times the number of awfuls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can plot this on an axis, there's the a.hree awfuls and one awesome,\n",
    "and for something that is all awesome, so three awesomes is at a point (3,0),\n",
    "and so on for other sentences \n",
    "Everything on one side we predict is positive,\n",
    "everything on the other we predict is negative.\n",
    "Now notice that the decision boundary,\n",
    "1.0 times awesome minus 1.5 times awful equals to 0 is a line.\n",
    "And so that's why it's called a linear classifier.\n",
    "It's a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So decision boundaries are what separate the positive\n",
    "predictions from the negative predictions.\n",
    "So, in the case of just two features, we see this is just a line.\n",
    "But that situation will differ as we increase the number of features.\n",
    "So in two dimensions, a linear function is a line.\n",
    "In three dimensions, we have three, words for example that have non-zero weight and\n",
    "everything else has zero weight and we get to the plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
