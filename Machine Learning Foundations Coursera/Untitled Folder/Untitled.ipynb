{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification of sentiment in a product review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A classifier takes some input x, it pushes it through what's called a model to output some value y, that we're trying to predict.\n",
    "In computer vision, we do a lot of classification. We'll take an image, and figure out what is in that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get a prediction whether it is a positive sentence or a negative sentence. In the sentimental analysis you can imagine a simple kind of threshold classifier. A threshold classifiers have some limitations.  how do we weigh different words? a linear classifier, instead of having a list of positive and negative words, actually takes all the words and adds weights to them. If you know the weight of each word, then it is a linear classifier because the output is basically the weighted sum of the input. Just weight, what features appears, what words appear in the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in summary, given a sentence and given the weights for\n",
    "the sentence, what we do is compute the score,\n",
    "which is the weighted count of the words that appear in the sentence.\n",
    "And then we say if the score is greater than zero,\n",
    "we predict y-hat to be positive.\n",
    "While if the score is less than zero, we predict it to be negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand decision boundaries,\n",
    "suppose you only had two words with non-zero weight.\n",
    "You have awesome with positive weight of one and\n",
    "awful which is just awful so it has a negative weight of 1.5.\n",
    "If you have this situation then the score is gonna be 1 times the number of\n",
    "awesomes in the sentence, minus 1.5 times the number of awfuls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can plot this on an axis, there's the a.hree awfuls and one awesome,\n",
    "and for something that is all awesome, so three awesomes is at a point (3,0),\n",
    "and so on for other sentences \n",
    "Everything on one side we predict is positive,\n",
    "everything on the other we predict is negative.\n",
    "Now notice that the decision boundary,\n",
    "1.0 times awesome minus 1.5 times awful equals to 0 is a line.\n",
    "And so that's why it's called a linear classifier.\n",
    "It's a linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So decision boundaries are what separate the positive\n",
    "predictions from the negative predictions.\n",
    "So, in the case of just two features, we see this is just a line.\n",
    "But that situation will differ as we increase the number of features.\n",
    "So in two dimensions, a linear function is a line.\n",
    "In three dimensions, we have three, words for example that have non-zero weight and\n",
    "everything else has zero weight and we get to the plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluating a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when I learn a classifier, I'm given a set of input data.\n",
    "So these are sentences that have been marked to say positive or negative\n",
    "sentiment, and as in regression, we split it into a training set and a test set.\n",
    "I feed the training set to the classifier I'm trying to learn and\n",
    "that algorithm is actually going to learn the weights for words.\n",
    "So for example it's going to learn that good has a weight 1.0.\n",
    "Awesome, 1.7.\n",
    "Bad, -1.0.\n",
    "And awful, -3.3.\n",
    "And then, these weights are going to be used to score every element in\n",
    "the test set and evaluate how good we're doing in terms of classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are goin to take sentences that we know their scores and\n",
    "feed it through the classifier, through the learned classifier.\n",
    "But we don't want the learned classifier to actually see the true label.\n",
    "We're gonna see if it gets the true label right.\n",
    "So we're gonna hide that true label.\n",
    "So the sentence gets fed to the learned classifier while the true label is hidden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count how many good or wrong preddictios our classifier made. So error measures, the fraction of the test examples that we make mistakes on.\n",
    "So what we just do is say, out of all of the sentences that are classified,\n",
    "how many mistakes there are made, so number of mistakes, and\n",
    "I divide that by the total number of test sentences. So for example if there were 100 test sentences and\n",
    "I made ten mistakes then our error would be 0.1 or 10%.\n",
    "Now the best possible error that I can make is zero basically,\n",
    "I make no mistakes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accuracy, instead of measuring the number of errors,\n",
    "we measure the number of correct classifications.\n",
    "So the ratio here is number\n",
    "of correct divided by total\n",
    "number of sentences.\n",
    "And unlike error where the best possible value is zero, in terms of accuracy,\n",
    "the best possible value is 1, I've got all the sentences right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's a good accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build a classifier,\n",
    "the first baseline comparison it should do is against random guessing. Majority class prediction \n",
    "its just predicted classes most common.\n",
    "And it can have amazing performance in cases where there's what's called class\n",
    "imbalance.\n",
    "One class has much more representation than the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False positives, false negatives, and confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is two kinds of mistakes that I can make.\n",
    "So, for example, if the true label's positive, but\n",
    "we predicted as negative, we call that a false negative. We said it was negative, but that was false cuz it's positive.\n",
    "Similarly, if the true label is negative when we predicted as positive,\n",
    "we call that a false positive.\n",
    "It was negative, but we predicted it as positive.\n",
    "And false positives and false negatives can have different impacts\n",
    "on what can happen in practice with your classifier. The relationship between the true label and the predicted label,\n",
    "false positive, false negatives, is called the Confusion Matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more data you have, the better,\n",
    "as long as the quality of the data is good.\n",
    "And then bad data, lots of bad data, is much worse to having much less,\n",
    "much fewer, data points with really good, clean, high-quality data points. Now there's some theoretical techniques to analyze how much data you need.\n",
    "Many of those help you understand kinda the overall trends, but\n",
    "really tend to be too loose to use in practice.\n",
    "In practice, there's some empirical techniques to really try to understand\n",
    "how much error we're making and what that kind of error looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a learning curve relates the amount of data that we have for\n",
    "training with the error that we're making.\n",
    "And here we're talking about test error.\n",
    "Now if you have very little data for training,\n",
    "then your test error is going to be high.\n",
    "But if you have a lot of data for training,\n",
    "your test error is going to be low.\n",
    "And now, the curve is gonna get better and\n",
    "better as you get more and more and more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex models tend to have less bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
